# UdacitySecureAndPrivateAI-60dayschallenge
Milestones of Udacity's #60dayschallenge


I create **2 side projects**: first is by myself and second is by a group. here are the links to repos:
- https://github.com/aksht94/UdacityOpenSource/tree/master/Richard
- https://github.com/aksht94/UdacityOpenSource/tree/master/SPAIC_Hungary


### Day 01. 30.06.2019.


First day of #60dayschallenge


Was no longer avaliable on Slack


### Day 02. 01.07.2019.


Was no longer avaliable on Slack


### Day 03. 02.07.2019.


Was no longer avaliable on Slack


### Day 04. 03.07.2019.


Was no longer avaliable on Slack


### Day 05. 04.07.2019.


Was no longer avaliable on Slack


### Day 06. 05.07.2019.


Was no longer avaliable on Slack


### Day 07. 06.07.2019.


Was no longer avaliable on Slack


### Day 08. 07.07.2019.


Was no longer avaliable on Slack


### Day 09. 08.07.2019.


Was no longer avaliable on Slack


### Day 10. 09.07.2019.


Was no longer avaliable on Slack


### Day 11. 10.07.2019.


Was no longer avaliable on Slack


### Day 12. 11.07.2019.


Was no longer avaliable on Slack


### Day 13. 12.07.2019.


Watching video on YouTube about DL networks:
https://youtu.be/y7qrilE-Zlc


### Day 14. 13.07.2019.


Watching two videos from YouTube about DL.
https://youtu.be/WCUNPb-5EYI


DL and snake
https://youtu.be/zIkBYwdkuTk


This is video teach almost nothing but gives an o testing example about using DL networks. 


Join to #SG-Hungary channel. 



### Day 15. 14 07.2019.
1. Reading article about DL tricks 
https://medium.com/cracking-the-data-science-interview/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1


2. Watching video class from Wolfram.
https://www.wolfram.com/wolfram-u/catalog/wl024/
If you are interested in learning from different sources, you must check this site. Udacity is much better, but hearing over and over again helps to get deeper knowledge. 


### Day 17. 16.07.2019.


I watched a video from other Udacity course by Luis Serrano.
https://youtu.be/BR9h47Jtqyw


I read an article from hackernoon.
https://hackernoon.com/%EF%B8%8F-big-challenge-in-deep-learning-training-data-31a88b97b282
It's not so good. Why I share this? Because this is a good lesson. Surfing on internet and searching good articles and papers always leads to dead end. Don't give up. If an article isn't good enough, you should make a list about what bothers you. After that you are able to refine the searching process to get more relevant matches.


### Day 18. 17.07.2019.


1. Organize local meetup with @Anita Goldpergel and @birozso. The event begin 20p.m. at Thursday. If you live in Hungary (Budapest), send me or @Anita Goldpergel to get address.


2. I read a paper about deep learning applications.
https://www.researchgate.net/publication/273063318_Deep_learning_applications_and_challenges_in_big_data_analytics


### Day 19. 18.07.2019.


1. Today the Hungarian Study Group took a seminar: @Anita Goldpergel, @birozso and @EveKov and me.


2. I joined to the #sg-europeers


So I spend the day building the community.


### Day 20. 19.07.2019.


I watched this video:
https://youtu.be/-6INDaLcuJY


I know, it's more than 1 hour, but you should watch it definitely. Good examples, understandable lecture. 


### Day 21. 20.07.2019


Completed lesson 8: Securing Federated Learning


Since I spent a lot of time about learning, I decided, I play with Jupiter notebook tomorrow.


### Day 22. 21.07.2019.


I begin the last lesson and played with the Jupiter notebook from lesson 8. 


### Day 23. 22.07.2019.


I read two articles about convolutional layers:


1.
https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/


2.
https://towardsdatascience.com/advanced-topics-in-deep-convolutional-neural-networks-71ef1190522d


### Day 24. 23.07.2019.


Today I decided to test my knowledge to watch a video from YouTube about DL. It's good to hear familiar things from different source. Here is the link:
https://youtu.be/Jy9-aGMB_TE


### Day 25. 24.07.2019


1. I played with Jupiter notebook files from lesson 8.


2. @susyjam and me try to motivate ourselves via private message on slack.


3. I read this article about different layer types. 
https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90


### Day 26. 25.07.2019.


Earlier I began a book named Deep Learning by Ian Goodfellow
https://www.deeplearningbook.org/


Today I read the 6th, 7th, and 8th chapters from part 2. This consume my whole free time, so I couldn't try it in Jupiter Notebook. 


### Day 27. 26.07.2019.


1. I continued the book by Ian Goodfellow. Today read the next chapters from part 2.


2. With @SG-Hungary we talked about organize new meetups, a virtual one and a face2face too.


### Day 28. 27.07.2019.


1. I finished the last lesson from this course. I decided to delay the practising at tomorrow.


2. @Anita Goldpergel, @birozso and me made a virtual meetup. We uploaded the photo in the fb group.


### Day 29. 28.07.2019.


I practised the yesterday learned lessons. Yesterday @birozso uploaded a pdf file in #SG-Hungary slack group about style transfer. I read that. 


### Day 30. 29.07.2019.


I read the lesson 13th and 14th from book by Ian Goodfellow. 


### Day 31. 30.07.2019.


I continued reading the book*. Today I learned lesson 15th and 16th.


* DL book by Ian Goodfellow


### Day 32. 31.07.2019.


I continued reading the book*. Today I learned lesson 17th.


* DL book by Ian Goodfellow


### Day 33. 01.08.2019.


I continued reading the book*. Today I learned lesson 18th.


* DL book by Ian Goodfellow


### Day 34. 02.08.2019.


1. I read this very good article about CNN layers:
https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53


2. I continued reading the book*. Today I learned lesson 19th.


* DL book by Ian Goodfellow


### Day 35. 03.08.2019.


I read the 20th chapter the DL book by Ian Goodfellow.



### Day 36. 04.08.2019.


Today @birozso, @EveKov and me made a virtual meetup, speaking and learning about style transfer.


Yesterday I went to bed early so I decided to upload my progress today. 


### Day 37. 05.08.2019.


Practice with Jupiter notebooks from SPAIC lessons. One of my favourite is the last, se I decided to play with that more. 


### Day 38. 06.08.2019.


Today I decided to dig deeper the differences between major DL frameworks, such as Pytorch, Keras, Tensorflow


I read an article and watch a video. Kinks are here:
https://towardsdatascience.com/keras-vs-pytorch-for-deep-learning-a013cb63870d


https://youtu.be/DmI58jz2i6w


### Day 39. 07.08.2019.


Today I decided to read articles that my fellow mates shared in this #60dayschallenge.


1. https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f
Link shared by @Jeremiah Kamama


2. https://medium.com/deep-math-machine-learning-ai/ch-14-general-adversarial-networks-gans-with-math-1318faf46b43
Link shared by @Alisha Javed


### Day 40. 08.08.2019. 
#60daysofudacity


Yesterday I liked the method to read shared articles so I decided toÂ  continue this method.


https://blog.openmined.org/encrypted-training-on-mnist/
Link shared by @Almina Assouane


http://bit.ly/art-with-neural-networks
Shared by @Tracy Adams


https://blog.openmined.org/federated-learning-of-a-rnn-on-raspberry-pis/
Shared by @Allaa Emad
This was fun since I have an old 
Rapsberry Pi at home. At my workplace we have a lot of RPi3 so I will try this method. We working with deep learning so I think my colleagues will happy to see this. 



### Day 41. 09.08.2019



My first article today was provided by @iso. about decision intelligence.
https://towardsdatascience.com/introduction-to-decision-intelligence-5d147ddab767


@Aleksandra Deis shared links about medical data visualisation. I like this topic so read this one:
https://towardsdatascience.com/basic-medical-data-exploration-visualization-heart-diseases-6ab12bc0a8b7


@Joanna Gon shared a link about NLP. 
https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ
This is very short but quite interesting. 



### Day 42. 10.08.2019.


Yesterday I watched video about NLP so I searched today new sources from this topic. @Sabrina shared an interesting video about emotion detection in text and connected DL stuff such as NLP.
https://youtu.be/mzw6zFZ29y4
I watched this, you should watch it too, if you are interested in NLP.


### Day 43. 11.08.2019


I continued browsing the shared content in challenge channel.


@Allex shared a very useful article about balancing the dataset.
https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb
I think this is useful either medical data too, since those are really imbalanced.


I watched this video about self driving cars and simulations.
https://youtu.be/EaY5QiZwSP4
Link shared by @Soumya Nasipuri
I think I'll try to build an oen model. It seems challenging and fun.


### Day 44. 12.08.2019


I continued browsing the shared contents. It's inspiring to see what other fellows read and watch. There are a lot of good and interesting content. 


This is a video about traffic identification:
https://youtu.be/yZ-Y1WCM0lc
Link shared by @Govind Dixit


I have to admit, I like Pokemons. That's why I started to read an article shared by @Anita Goldpergelnkit Dubey
https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/
I tried Keras earlier with CNN. I think using Keras in a beginner stage is easier than Pytorch. Maybe it runs smoother and a little bit faster. Okay, I know it depends on the size of layers and nodes. So this article was interesting, but in my opinion is using Pytorch is better and cooler. Pytorch gives more freedom during building the networks. This article makes stronger my viewpoint using Pytorch.


PS: If you have a little free time, you definitely should build a model in Keras and Pytorch simultaneously. 


### Day 45. 13.08.2019


I'm working on my final projec/showcase. That's why I was happy seeing @Manira Omar's post about hand detection:
https://medium.com/@soffritti.pierfrancesco/handy-hands-detection-with-opencv-ac6e9fb3cec1


The idea is good, I will use hand detection too, but unfortunately this article contains too much rules. Not moving user, static camera, constant background. This project is far from reality and real life situations.


The next article about GAN was shared by @Mubashir
https://machinelearningmastery.com/resources-for-getting-started-with-generative-adversarial-networks/
It's quite interesting, everybody should read this since it contains a lot of examples. 


### Day 46. 14.08.2019.


I played with Jupiter notebooks from lessons 6 and 7 and read an article shared by @Maria about classifiers:
https://arxiv.org/abs/1602.04938


### Day 47. 15.08.2019.


Today I coded my final project and watched the video about deep neural networks
https://youtu.be/_H3aw6wkCv0


### Day 48. 16.08.2019.


Today I wrote the code of my final project and watched the first third part of this video:
https://youtu.be/CNuI8OWsppg
It's more than 5 hours, so I decided to cut in 3 parts.


### Day 49. 17.08.2019


Today I coded my final challenge. I tried to reinstall pysydt and I got a lot of errors. If you need help to install, don't hesitate to contact me. Now the installation of pytorch, torchvision and pysyft in pip and conda channels are not compatible with each others. The key is choosing earlier versions in a new environment.


Since coding and installing consumed a lot of time I didn't watch the next part of video that I began yesterday. Tomorrow I will continue. 


### Day 50. 18.08.2019.


I finished the coding process of my showcase, that is my keystone project too. Keystone challenge was mentioned in lesson 9 chapter 8. I published it on official github channel and Facebook group. 



### Day 51. 19.08.2019.


I continued the second parts of this video that was began at 48th day:. https://youtu.be/CNuI8OWsppg


### Day 52. 20.08.2019.


Our #sg-hungary team coded the common showcase today. This project is about style transfer. 


### Day 53. 21.08.2019.


I finished this video:
https://youtu.be/CNuI8OWsppg
Since the last part was nearly 2 hours and I didn't have enough free time, this was the only program today.


### Day 54. 22.08.2019.


Today I read about optimizing the training process.
This is a very useful article:
https://sagivtech.com/2017/09/19/optimizing-pytorch-training-code/


If you want to see under the hood:
https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051


In my opinion paralellization seems very promising. However, building a paralellization-ready machine could consume a lot of money. 


### Day 55. 23.08.2019.


I read about DL model building based on the architecture of computer:
https://medium.com/@colinshaw_36798/fully-utilizing-your-deep-learning-gpus-61ee7acd3e57


The second article contains good tips on this topic too:
https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255


### Day 56. 24.08.2019.


I read an essay about implementation of Stochastic Quasi-Newton's Method in PyTorch 
https://arxiv.org/abs/1805.02338
For this you need a little bit more mathematical skill than you had to in this course.


Since yesterday I read about architecture, the second article was connected to this topic: adaptive batch sizes for training.
https://arxiv.org/abs/1712.02029
They get less training time with relative 1% accuracy changing using smaller batches. It sounds good, but if you check the diagrams on the 7th page, you see the performance increasing is better with small epochs. After 60 epochs the advantage fades away.


### Day 57. 25.08.2019.


I revisited the 4th, 5th, and 6th lesson in SPAIC course. This revision based on playing JupyterNotebook.


I think this #60dayschallenge was a good idea since the rate of forgetting is lower, than other course without challenge. Besides of this I learn much more from other, connected field of ML. I will use this method in the future. 


### Day 58. 26.08.2019.


I started watch video lecturee from MIT about DL:
https://youtu.be/JN6H4rQvwgY
I know, this course will end before I'am able to watch all of episodes, but I will move forward after this course. It's better not to stop this challenge-based learning methods, so I decided I start to watch and learn videos and articles that give me knowledge after this challenge.


### Day 59. 27.08.2019.


My first choice was driven by fun. I read an article about ML examples.
https://www.forbes.com/sites/bernardmarr/2018/04/30/27-incredible-examples-of-ai-and-machine-learning-in-practice/


I know, Forbes may be a good sources from finance and business field, but not good if we speak about AI. However, it's interesting to see what people and business journalist think about ML.


The second article came from business too: AI and portfolio management to reduce risk and maximize profit
https://medium.com/sakyatech/artificial-intelligence-driven-portfolio-management-a-view-of-the-landscape-93bba2caab4e


My first degree is finance so this is interesting topic to me. These codes work till the bigger part of market doesn't use it. When the most decisions is based on AI, the new training sets get a lot of unnatural data. I mean natural data represents human decisions and unnatural data contains decisions from other AIs. With the good loss function the trend go to the median. In my opinion, portfolio management is not a classical ML task. 


### Day 60. 28.08.2019


Hurray, I meet the deadline. :) This gave me an interesting idea: I try to widen my knowledge on the old way, learning books. Real books. So I decided to search ML books. Here is a good selection:
https://hackernoon.com/the-best-machine-deep-learning-books-e1bcec2c0a17
After this course I will order one or maybe more of them.


Despite of this is the last day, the show must go on. I decided to learn about making smaller NL models with better performance.
https://www.technologyreview.com/s/613514/a-new-way-to-build-tiny-neural-networks-could-create-powerful-ai-on-your-phone/
Interesting theory to train bigger networks and cut out neurons after the train process to get tiny networks.

This article gave examples from the area of pruning:
https://jacobgil.github.io/deeplearning/pruning-deep-learning


### Day 61. 29.08.2019 


I read an article and watch video by LG from CES 2019
https://www.digitaleurope.org/news/case-studies-on-artificial-intelligence/
I anticipated, that video is a huge advertise, but it contains a lot of interesting ideas. Okay, after Udacity's course you won't learn anything from it, but it gives inspiration.


The second video was fantastic. It's an MIt class called The Ethics and Governance of AI Course, Class 2
https://youtu.be/AjYnsG7zVwQ


### Day 62. 30.08.2019.


Today I read an "old" paper about face detection 
https://www.researchgate.net/publication/221364881_Neural_Network-Based_Face_Detection
Watch the date! :)


Besides this I watched the repository of showcase challenge again. It made me motivated to see these great codes and a huge amount of energy and work from fellow students. I am proud to be a part of it.

